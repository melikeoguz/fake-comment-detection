{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">Makine Öğrenme Yöntemleriyle Sahte Kullanıcı Yorumlarının Tespiti</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.hizliresim.com/b7h62o.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Proje İçeriği</h3>\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>1- Giriş</li>\n",
    "<li>2- Data Setinin Oluşturulması</li>\n",
    "<li>3- Proje İçin Kullanılan Yöntemler</li>\n",
    "<li>3.1- LSTM</li>\n",
    "<li>3.2- Lojistik Regrasyon</li>\n",
    "<li>4- Kullanılan Algoritmaların Karşılaştırılması (Sonuç)</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1- Giriş</h2>\n",
    "\n",
    "E-ticaret sistemi çok geliştiği için ürünler hakkında birçok yorum yapılmaktadır. Yapay zekanın gelişmesiyle birlikte verilerin gerçekliği hakkında şüpheler başlamıştır. Bu projede bu soruna çözüm aranmaktadır. Geliştirilen sistem şu şekilde çalışmaktadır. Öncelikle klavyeden bir giriş alınır. Daha sonra eğitilmiş modelimiz bunun makine yorumu mu (sahte yorum) yoksa insan yorumu mu olduğunu tespit etmektedir.\n",
    "\n",
    "\n",
    "<h2>2- Veri Setinin Oluşturulması</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data setinin türkçe olması için datasetini kendimiz oluşturduk. Datasetinde toplamda 2549 adet yorum bulunmaktadır. Bu verilerin <b>1200</b> tanesi gerçek kullanıcı yorumudur. Geriye kalan veriler fake olarak üretilmiştir. Fake yorumlar ise GRU (Lstm nöron çeşiti) ile geliştirilmiştir. https://github.com/Aksoylu adresindeki bir proje kaynak alınarak 1349 adet fake veri üretilmiştir.\n",
    "\n",
    "Doğal yorumlar hepsiburada'da yer alan ürün yorumlarından toplanmıştır. Genellikle maskara, ayakkabı, çanta, telefon, kulaklık gibi ürünlerin yorumlarından oluşmuştur. Datasetimizi oluştururken <b>Kullanıcı yorumları 1</b> ile ifade edilirken <b>fake yorumlar 0</b> ile gösterilmiştir. Bunun için oluşturulan datasetinde <b>yorum</b> ve <b>etiket</b> sütunları bulunmaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\melike\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (4.9.3)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\melike\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\melike\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.hepsiburada.com/maybelline-new-york-lash-sensational-sky-high-maskara-p-HBV00001B1F33-yorumlari?magaza=AtakanKozmetika&sayfa=1\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ürün gerçekten güzel ve kalite hissini veriyor.\n",
      "Ekran çok güzel ıps olduğundan ama parlaklık biraz daha iyi olabilirdi dışarıda güneşte en son parlaklıkta kullanmak gerekecek.\n",
      "Hafıza gerçekten yeterli.\n",
      "Oyun oynatıyor çok aşırı olmadıkça yani gta 5 veya csgo çok rahat oynanıyor işlemcisi buna çok müsait.\n",
      "Açılıp kapanma hızı gerçekten çok güzel.\n",
      "Hafıza yeterli.\n",
      "Ram hızı 3200 bilgisayarlara bakarken buna da bakın arkadaşlar genelde ram hızı 2600 olur bunda 3200 bu güzel bir avantaj.\n",
      "Ram değişmiyor bilginiz olsun.\n",
      "Klavye ortalama üstü ama çok iyi değil.\n",
      "Malzeme kalitesi olarak güzel kullanırken sıradan bir ürün olmadığını gösteriyor.\n",
      "Genel olarak fiyatının hakkını sonuna kadar veriyor bu fiyata daha iyisi yok. Alacaklara hayırlı olsun.\n",
      "tek eksik hissettiğim belki klavye ışığı olabilirdi. gece çalışırken rahat olurdu. laptop süper , tam bir kompakt , hızlı , heryere taşınabilir. 4k görüntü veriyor hdmı ile tv ye. tavsiye ederim.\n",
      "Öncelikle kargo çok hızlı geldiğini belirtmek istiyorum. Son sınıf öğrencisiyim, bu laptopta oyun oynama niyetim yok ama düşük ayarlarda oynayabilirsiniz. Çok hızlı, güzel, ve sessiz. Ben memnun kaldım. Ekran kalitesi iyi. Kamera kapatma özelliği on numara olmuş. \n",
      "Ofis ve okul için 10/10 \n",
      "Ses kalitesi tatmin edici. Klasik şarj girişi ve type-C var (yani ikisi de şarj eder)\n",
      "Laptop güçlü, dışı çok lüks bir malzemelerden yapılmadığı için,  14 inçe sığabilecek maksimum güç koydular yani. \n",
      "Satıcıyı öneriyorum. Teşekkür ederim.\n",
      "Ürünü uzun araştırmalarım sonucu aldım, 2 gün sonra yani belirtilen sürede elime geçti, öncelikle teşekkürler.\n",
      "Ürün, örnek görsellerden çok daha kaliteli, şık ve zarif duruyor. Premium bir his veriyor. Bu kadar beklemiyordum açıkçası görsellerde düz plastik duruyor. SSD ile aletin açılması birkaç saniye sürüyor. Ekran gayet iyi, IPS en büyük tercih sebebimdi. Tavsiye ediyorum kesinlikle bu fiyata alınabilecek en iyi seçenek. 2021 çıkışlı olan ve ryzen 7 işlemciye sahil bir aletin bu fiyata sunulması şu dönemde şans\n",
      "Gereğinden fazla iyi her anlamda. Ram değişememesi kötü ama dual channel olduğu için ve oyun için almadığımdan dolayı hiç bir sorun teşkil etmiyor bana. Kullanılan ssd gereğinden fazla iyi. Kısacası iş ve öğrenciler için bulunmaz nimet.\n",
      "İyi ürün hızlı kargo\n",
      "Her yönüyle harika\n",
      "Ürün mükemmel kargolama güzel hızlı gönderim için teşekkürler\n",
      "Çok memnun kaldım\n",
      "Sorunsuz teslimat teşekkürler\n",
      "Çok laptop araştırdım bu fiyat aralığında, emsallerine göre çok daha iyi ve çok daha uygun fiyatlı bir laptop.\n",
      "\n",
      "Artıları,\n",
      "Ürünün altında havalandırma kanallarının olması da çok iyi düşünülmüş , ucuz bir soğutucu bile çok kolay sıcaklığı 15 derece düşürebiliyor, ürün soğutucu olmadan kullansanız bile çok normal değerlerde kalıyor sıcaklığı\n",
      "\n",
      "işlemcisi ryzen 7,5700u yeni çıkmış çok iyi bir işlemci, intel 7 seviyesine denk geliyo,ekran kartı amd'de tümleşik olduğu için tereddütlerim vardı ama csgo 'dan düşük ayarlarda 150 fps-200fps alabildiğimi görünce mutlu oldum\n",
      "\n",
      "Eksileri,\n",
      "8gb Ram gömülü, arttırılamıyor, ancak ramin 3200 mhz olarak çalışması çok iyi\n",
      "\n",
      "Laptop'un 14 inç ve çok ince olması her türlü çantaya rahatlıkla sığabilmesi için iyi bi şey fakat bana biraz küçük gelmişti laptop , şimdi alıştım altına soğutucu da koyunca zaten hiç küçük gelmiyo\n",
      "\n",
      "Işıklı klavye yok, benim için çok önemli bi şey değil zaten\n",
      "Metalik gri çok yakışıklı bir kasa bu işlemciye göre süper fiyat. Ders için alındı Oyun için gidermi bilmem ama tam aradığım hafif kibar hızlı\n",
      "Sağlam ve hızlı kargo gundelik kullanim icin ideal bir laptop hoparlor kalotesi vs cok iyi\n",
      "Hayatımda aldığım en hızlı kargo oldu paketleme her şey sorunsuzdu :)\n",
      "Ürün parasına göre ve muaddilerine göre iyi. İşlemci hafıza hızlı. Hafif ve kompakt bir ürün büyük oranda memnun kaldım. Bana göre 3 tane eksi yani var. İlki ram gömülü 8 GB artiramiyorsunuz. Ethernet port girişi yok ve 4k televizyona görüntü aktarımında harici ekran kartı olmadigi için takılma yapabiliyor\n",
      "Görüntü idare eder\n",
      "Fiyatına göre ideal yeterli ben beğendim şuan sorun yok\n",
      "Aldıktan bir gün sonra indirim oranını yükseltmek alan kişiye büyük saygısızlık. Yazıcıyı başka siteden aldım bu yaptığınız davranıştan dolayı.\n",
      "Bilgisayar elime ulaşalı 2 saat oldu ve şimdi ekranda ölü pixel olduğunu farkettim. Klavyede esc tuşu nedense yerinden çıkıcak gibi duruyor touchpadi çalışıyor fakat sol tarafı kutudan çıktığından beri içine doğru çökük duruyor. Ürün görsellerinde klavye aydınlatması görünmesine rağmen elime gelen laptopta mevcut değil. Bu bahsettiğim sıkıntılara denk gelmezseniz güzel bilgisayar eline ulaştığı gibi kontrol etmeyi unutmayın.\n",
      "Karga müthiş hızlı... Paketleme muhteşem. Ürün anlatıldığı gibi fiyat performans ürünü gayet kaliteli\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.hepsiburada.com/lenovo-ideapad-5-amd-ryzen-7-5700u-8gb-512gb-ssd-freedos-14-fhd-tasinabilir-bilgisayar-82lm006etx-p-HBCV00000471GC-yorumlari?sayfa=1\"\n",
    "headersparam = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}\n",
    "r = requests.get(link, headers=headersparam)\n",
    "soup = BeautifulSoup(r.content, \"html\", from_encoding='UTF-8')\n",
    "span = soup.find_all(\"span\",{\"itemprop\": \"description\"})\n",
    "\n",
    "for i in span:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinin Boyutu: \n",
      "(2549, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Yorum</th>\n",
       "      <th>Etiket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>maskara ürünü desteklemeyen yerlir görmedim yi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>maskarafı öne kullanıyorum yada fark atıyor\\r\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>maskarada bulunan dokunma diğeri  kere iti akt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>maskara girmesini beklemeyin üst seviye\\r\\ngüz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>maskara kıyasla uygun fiyatlı olması kolay ay ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>mousekablo derdinden kurtuldum henüz yukarı ka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>mousekablo uyumu vardır amcam tane almıştım ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>mouse bişey imi kaçırmasın\\r\\ngüzel\\r\\nsadece ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>mouseleri ergonomik bir telefon şimdiden almam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>mouse vb hediye haksız biraz korkularım yanınd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              Yorum  Etiket\n",
       "0             0  maskara ürünü desteklemeyen yerlir görmedim yi...       0\n",
       "1             1  maskarafı öne kullanıyorum yada fark atıyor\\r\\...       0\n",
       "2             2  maskarada bulunan dokunma diğeri  kere iti akt...       0\n",
       "3             3  maskara girmesini beklemeyin üst seviye\\r\\ngüz...       0\n",
       "4             4  maskara kıyasla uygun fiyatlı olması kolay ay ...       0\n",
       "..          ...                                                ...     ...\n",
       "995         995  mousekablo derdinden kurtuldum henüz yukarı ka...       0\n",
       "996         996  mousekablo uyumu vardır amcam tane almıştım ha...       0\n",
       "997         997  mouse bişey imi kaçırmasın\\r\\ngüzel\\r\\nsadece ...       0\n",
       "998         998  mouseleri ergonomik bir telefon şimdiden almam...       0\n",
       "999         999  mouse vb hediye haksız biraz korkularım yanınd...       0\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv') #Oluşturulan verisetinin gösterilmesi\n",
    "print(\"Verisetinin Boyutu: \")\n",
    "print(data.shape)\n",
    "data.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3- Proje İçin Kullanılan Algoritmalar</h2>\n",
    "\n",
    "Bu projede iki adet algoritma kullanılmıştır. Bunlardan bir tanesi <b>LSTM</b> diğeri ise <b>Lojistik Regrasyondur</b>. İki adet algoritma kullanılma sebebi ise hangi algoritmanın daha iyi sonuç vereceğini tespit etmektir.\n",
    "Bu algoritmaların karşılaştırılması ve sonuçları ilgili kısımlarda anlatılacaktır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM yapısı itibariyle bir model eğitilirken zaman almaktadır. Bu algoritmayı tercih etmemizin sebebi ise daha iyi öğrenme gerçekleşeceğini ilgili makaleleri incelememizdir. Biz de denemek için öncelikle sistemimizi bu şekilde geliştirdik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1- LSTM (Long-Short Term Memory) Algoritması</h3>\n",
    "    \n",
    "Öncelikle bilindiği üzere LSTM derin öğrenme algoritmalarından en popüleri olan RNN(Recurrent Neural Network)'nin uzun süreli bağımlılıkları öğrenebildiği özel bir türüdür. \n",
    "\n",
    "<h3>RNN Nedir ?</h3>\n",
    "    \n",
    "Rnn'ler bir sonraki adımı tahmin etmek için kullanılan bir çeşit Derin Öğrenme Yapılarıdır. Diğer Derin Öğrenme Yapılarından en büyük farkları ise <b>hatırlamalarıdır.</b> Diğer bir farkı ise girdilerin birbirleri ile ilişkili olmasıdır. RNN’ler bir sonraki adımı takip edebilmek için girdiler arasında ilişkiler kurarlar ve eğitilirken tüm ilişkilerini hatırlarlar.\n",
    "\n",
    "RNN’ler kurmuş oldukları ilişkilerin kalıcı olması için kendi içlerinde dönen döngü benzeri bir yapı kullanırlar. Tekrarlayan sinir ağları döngülere sahiptir. Aslında çıkan sonuç, bir sonrakini besliyor. Bu yapının açık hali;\n",
    "\n",
    "<h4>RNN Yapısı :</h4>\n",
    "\n",
    "<img src=\"https://i.hizliresim.com/3d7w94.png\">\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzun-Kısa Süreli Bellek Ağları, uzun süreli bağımlılıkları öğrenebilen özel bir RNN türüdür. Ayrıca uzun kısa süreli bellek derin öğrenme alanında kullanılan yapay bir tekrarlayan sinir ağı mimarisidir. Standart ileri beslemeli sinir ağlarının aksine, LSTM'nin geri bildirim bağlantıları vardır. Yalnızca tek veri noktalarını değil, aynı zamanda tüm veri dizilerini işleyebilir. \n",
    "LSTM lerin tasarlanma amacı uzun vadeli bağımlılık sorununu önlemektir. Varsayımsal olarak bilgiyi uzun süreler boyunca hatırlarlar.\n",
    "\n",
    "Tüm tekrarlayan sinir ağları, sinir ağının tekrar eden modüllerinin bir zinciridir. \n",
    "\n",
    "Standart bir RNN’deki tekrarlayan modül tek bir katman içerir. LSTM’ler de bu zincir benzeri yapıya sahiptir, ancak tek bir nöral ağ katmanı yerine 4 tanesine sahiptir.\n",
    "\n",
    "<b>Ayrıca LSTM'in bir katında 128 adet bellek hücresi bulunur.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM Yapısı</h3>\n",
    "\n",
    "<img src=\"https://i.hizliresim.com/3vqfb3.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Forget Gate (Unutma Kapısı):</b>\n",
    "Hangi bilginin tutulacağı veya unutulacağına karar verir. Mantığı hiç karmaşık değil. Matematikte öğrendiğimiz gibi, bir sayı 0 ile çarpılırsa ne kadar büyük olursa olsun sonuç 0 olur. Burada da aynı mantıkla işlem yapılıyor. Unutmak için girdinin ağırlığına 0 verilir.\n",
    "Bir önceki gizli katmandan gelen bilgiler ve güncel bilgiler Sigmoid Fonksiyonundan geçer. 0'a ne kadar yakınsa o kadar unutulacak, 1'e ne kadar yakınsa o kadar tutulacak demektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input Gate (Girdi Kapısı):</b>\n",
    "Cell State’i güncellemek için kullanılır. Öncelikle Forget Gate’de (Unutma Kapısı) olduğu gibi Sigmoid fonksiyonu uygulanır, hangi bilginin tutulacağına karar verilir. Daha sonra ağı düzenlemek için Tanh fonksiyonu yardımıyla -1,1 arasına indirgenir ve çıkan iki sonuç çarpılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cell State:</b>\n",
    "Cell State’in hücre içerisindeki en önemli görevi bilgiyi taşımaktır. Taşınması gereken verileri alır ve hücre sonuna, oradan da diğer hücrelere taşır. Yani ağ üzerinde veri akışını Cell State yardımıyla sağlarız. İlk olarak Forget Gate’den (Unutma Kapısı) gelen sonuç ile bir önceki katmanın sonucu çarpılır. Daha sonra Input Gate’den (Girdi Kapısı) gelen değer ile toplanır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Output Gate (Çıktı Kapısı):</b>\n",
    "Bir sonraki katmana gönderilecek değere karar verir. Bu değer, tahmin için kullanılır. Öncelikle bir önceki değer ile şu anki girdi Sigmoid fonksiyonundan geçer. Cell State’den gelen değer Tanh fonksiyonundan geçtikten sonra iki değer çarpılır ve bir sonraki katmana “Bir önceki değer” olarak gider. Cell State ilerler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM Trainer Kısmı</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install keras\n",
    "#pip3 install tensorflow\n",
    "#pip3 install nltk\n",
    "#pip3 install gensim\n",
    "\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.models import Sequential,load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset.head()\n",
    "dataset.sort_values(\"Body\", inplace = True)\n",
    "dataset = dataset.drop(columns=\"B\")\n",
    "\n",
    "dataset.drop_duplicates(subset =\"Body\",keep = False, inplace = True)\n",
    "                     \n",
    "\n",
    "def optimizasyon(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "\n",
    "    for ind in dataset.index:\n",
    "        body = dataset['Body'][ind]\n",
    "        body = body.lower()\n",
    "        body = re.sub(r'http\\S+', '', body)\n",
    "        body = re.sub('\\[[^]]*\\]', '', body)\n",
    "        body = (\" \").join([word for word in body.split() if not word in stop_words])\n",
    "        body = \"\".join([char for char in body if not char in noktalamaIsaretleri])\n",
    "        dataset['Body'][ind] = body\n",
    "    return dataset\n",
    "\n",
    "dataset = optimizasyon(dataset)\n",
    "\n",
    "\n",
    "X = dataset.loc[:,\"Body\"]\n",
    "y = dataset.loc[:,\"Label\"]\n",
    "\n",
    "print(X)\n",
    "\n",
    "\n",
    "print(y)\n",
    "\n",
    "X_egitim, X_test, y_egitim, y_test = train_test_split(X, y, test_size = 0.2, random_state = 28) \n",
    "\n",
    "# Word2Vec yani dökümanı vektöre çevirecek gensim modeli için gerekli parametreler\n",
    "maxmesafe = 2 #Bir cümle içindeki mevcut ve tahmin edilen kelime arasındaki maksimum mesafe\n",
    "minfrekans = 1 #Toplam sıklığı bundan daha düşük olan kelimeleri göz ardı eder\n",
    "vektor_boyut = 200 #Öznitelik vektörlerinin boyutluluğu\n",
    "maxlen = 1000 #Bir  metninin maksimum uzunluğu\n",
    "\n",
    "\n",
    "X_egitim_splited = [metin.split() for metin in X_egitim]\n",
    "w2v_model = gensim.models.Word2Vec(sentences = X_egitim_splited, vector_size=vektor_boyut, window = maxmesafe,  min_count = minfrekans)                                          \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_egitim_splited)   #vektore donüştürme\n",
    "X_egitim_tok = tokenizer.texts_to_sequences(X_egitim_splited)\n",
    "kelime_index = tokenizer.word_index\n",
    "kelime_sayi = len(kelime_index) + 1\n",
    "X_egitim_tok_pad = pad_sequences(X_egitim_tok, maxlen=maxlen)  #butun x eğitim verilerini vektore donusturur\n",
    "\n",
    "\n",
    "print('Sözlük boyutu: ', kelime_sayi) #toplam kac tane farklı kelime varsa bilgisini verir\n",
    "\n",
    "matris = np.zeros((kelime_sayi, vektor_boyut))\n",
    "for kelime, i in kelime_index.items():\n",
    "    matris[i] = w2v_model.wv[kelime]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(matris.shape[0], \n",
    "                    output_dim=matris.shape[1],\n",
    "                    weights=[matris], \n",
    "                    input_length=maxlen, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(units=32))   \n",
    "model.add(Dense(1, activation='sigmoid'))   # Aktivasyon fonksiyonu olarak \"Sigmoid\" i seçiyoruz\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])    # Optimizer parametresi olarak \"adam\", loss için ise \"binary_crossentropy\" seçiyoruz.\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(X_egitim_tok_pad, y_egitim, validation_split=0.2, epochs=30, batch_size = 64, verbose = 1)\n",
    "\n",
    "model.save('egitilmis_model.h5')\n",
    "\n",
    "print(\"Model eğitildi ve kayıt edildi !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM Prediction Kısmı</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "maxlen = 1000\n",
    "\n",
    "# Bir metini stopword kelimelerden, noktalama işaretlerinden, linklerden ve gereksiz yapılardan temizleyen optimizasyon fonksiyonu.\n",
    "# Yapay zeka tarafından yorumlanacak veri ilk olarak bu aşamadan geçer\n",
    "def optimizasyon(metin):\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "    metin = metin.lower()\n",
    "    metin = re.sub(r'http\\S+', '', metin)\n",
    "    metin = re.sub('\\[[^]]*\\]', '', metin)\n",
    "    metin = (\" \").join([word for word in metin.split() if not word in stop_words])\n",
    "    metin = \"\".join([char for char in metin if not char in noktalamaIsaretleri])\n",
    "    return metin\n",
    "\n",
    "# Tokenizer sınıfı kullanılarak düz metin, yapay zekanın yorumlayacağı hale getirilir.\n",
    "def metinDonustur(testData,dataset):\n",
    "    testData_splited = [testData.split() ]                              \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(testData_splited)\n",
    "    testData_tok = tokenizer.texts_to_sequences(testData_splited)\n",
    "    kelime_index = tokenizer.word_index\n",
    "    kelime_sayi = len(kelime_index) + 1\n",
    "    testData_tok_pad = pad_sequences(testData_tok, maxlen)\n",
    "    text_test_tok = tokenizer.texts_to_sequences(dataset.loc[:,\"Body\"])\n",
    "    text_test_tok_pad = pad_sequences(text_test_tok, maxlen=maxlen)\n",
    "    print(text_test_tok_pad)\n",
    "    return testData_tok_pad\n",
    "\n",
    "\n",
    "def datasetOptimizasyon(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "\n",
    "    for ind in dataset.index:\n",
    "        body = dataset['Body'][ind]\n",
    "        body = body.lower()\n",
    "        body = re.sub(r'http\\S+', '', body)\n",
    "        body = re.sub('\\[[^]]*\\]', '', body)\n",
    "        body = (\" \").join([word for word in body.split() if not word in stop_words])\n",
    "        body = \"\".join([char for char in body if not char in noktalamaIsaretleri])\n",
    "        dataset['Body'][ind] = body\n",
    "    return dataset\n",
    "\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "dataset = datasetOptimizasyon(dataset)\n",
    "\n",
    "model = load_model(\"egitilmis_model.h5\")\n",
    "\n",
    "\n",
    "def tahminEt(inputData):\n",
    "\n",
    "    inputData = optimizasyon(inputData)\n",
    "    testData = metinDonustur(inputData,dataset)\n",
    "\n",
    "    print(\"haber: \\n\" + inputData)\n",
    "    pred = model.predict([testData]) \n",
    "    print(pred)\n",
    "    if (pred > 0.98):\n",
    "        print(\"Bu yorum insan üretimi olarak tahmin edilmiştir\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Bu yorum makine üretimi (sahte) olarak tahmin edilmiştir.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "haber = input(\"Yorum girin\")\n",
    "tahminEt(haber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM Algoritması ile Sistemin Çalıştırılma Çıktıları</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/4.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/5.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/6.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/8.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2- Lojistik Regrasyon Algoritması</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lojistik regresyon,</b> bir sonucu belirleyen bir veya daha fazla bağımsız değişken bulunan bir veri kümesini analiz etmek için kullanılan istatistiksel bir yöntemdir. \n",
    "\n",
    "<b>Sonuç, ikili bir değişkenle ölçülür (yalnızca iki olası sonuç vardır). Yani 1 (Doğru, başarı), ya da 0 (Yanlış, hata) olarak kodlanmış verileri içerir.</b>\n",
    "\n",
    "Amacı, iki yönlü karakteristiği (bağımlı değişken = yanıt veya sonuç değişkeni) ile ilgili bir dizi bağımsız (öngörücü veya açıklayıcı) değişken arasındaki ilişkiyi tanımlamak için en uygun (henüz biyolojik olarak makul) modeli bulmaktır. \n",
    "\n",
    "Lojistik regresyon, ilgi karakteristiklerinin varlığının olasılığını logit dönüşümünü tahmin etmek için bir formülün katsayılarını (ve standart hatalarını ve önem seviyelerini) üretir.\n",
    "\n",
    "* Projemizde iki adet parametre olduğu için yani ya sahte yorum ya da makine yorumu diye karar verileceği için bu algoritmanın kullanılmasının uygun olacağına karar verdik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lojistik Regrasyon Train Kısmı</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install pickle-mixin\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "#Veri setini oku\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset.head()\n",
    "dataset.sort_values(\"Body\", inplace = True)\n",
    "dataset = dataset.drop(columns=\"B\")\n",
    "\n",
    "#Aynı verileri sil\n",
    "dataset.drop_duplicates(subset =\"Body\",keep = False, inplace = True)\n",
    "\n",
    "\n",
    "#Optimizasyon Fonksiyonu\n",
    "def optimizasyon(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "\n",
    "    for ind in dataset.index:\n",
    "        body = dataset['Body'][ind]\n",
    "        body = body.lower()\n",
    "        body = re.sub(r'http\\S+', '', body)\n",
    "        body = re.sub('\\[[^]]*\\]', '', body)\n",
    "        body = (\" \").join([word for word in body.split() if not word in stop_words])\n",
    "        body = \"\".join([char for char in body if not char in noktalamaIsaretleri])\n",
    "        dataset['Body'][ind] = body\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = optimizasyon(dataset)\n",
    "\n",
    "\n",
    "yorumlar_makina = dataset[dataset['Label']==0]\n",
    "yorumlar_insan = dataset[dataset['Label']==1]\n",
    "\n",
    "tfIdf = TfidfVectorizer( binary=False, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "makina_cv = tfIdf.fit_transform(yorumlar_makina['Body'].tolist())\n",
    "insan_cv = tfIdf.fit_transform(yorumlar_insan['Body'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "X = dataset['Body']\n",
    "y = dataset['Label']\n",
    "\n",
    "x_tv = tfIdf.fit_transform(X)\n",
    "\n",
    "x_train_tv, x_test_tv, y_train_tv, y_test_tv = train_test_split(x_tv, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "log_tv = LogisticRegression() \n",
    "log_tv.fit(x_train_tv,y_train_tv)\n",
    "\n",
    "pickle.dump(log_tv, open(\"egitilmis_model\", 'wb'))\n",
    "print(\"Lojistik Regresyon modeli eğitildi ve kayıt edildi !\")\n",
    "\n",
    "\n",
    "pickle.dump(tfIdf, open(\"vektorlestirici\", 'wb'))\n",
    "print(\"Tf-Idf vektörleştirici modeli kayıt edildi !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/1-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lojistik Regresyon Prediction Kısmı</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimizasyon(metin):\n",
    "\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "\n",
    "    body = metin\n",
    "    body = body.lower()\n",
    "    body = re.sub(r'http\\S+', '', body)\n",
    "    body = re.sub('\\[[^]]*\\]', '', body)\n",
    "    body = (\" \").join([word for word in body.split() if not word in stop_words])\n",
    "    body = \"\".join([char for char in body if not char in noktalamaIsaretleri])\n",
    "    return body\n",
    "\n",
    "\n",
    "LogisticRegressionModel = pickle.load(open(\"egitilmis_model\", 'rb'))\n",
    "\n",
    "tfIdf = pickle.load(open(\"vektorlestirici\", 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "def tahminEt(testData):\n",
    "    testData = optimizasyon(testData)\n",
    "    vektorlestirilmis_test_verisi = tfIdf.transform([testData])\n",
    "\n",
    "\n",
    "    print(vektorlestirilmis_test_verisi)\n",
    "    #4 yıldız çünkü jelatini açılmıştı jelatininin açılması dışında her şey yolundaydı\n",
    "    tahminSonuc = LogisticRegressionModel.predict(vektorlestirilmis_test_verisi)\n",
    "\n",
    "    print(tahminSonuc)\n",
    "\n",
    "\n",
    "\n",
    "inp = input(\"Yorum giriniz\")\n",
    "\n",
    "\n",
    "tahminEt(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lojisik Regrasyonun Doğruluğunu Ölçme</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install pickle-mixin\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "#nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset.head()\n",
    "dataset.sort_values(\"Body\", inplace = True)\n",
    "dataset = dataset.drop(columns=[\"B\",\"id\"])\n",
    "\n",
    "dataset.drop_duplicates(subset =\"Body\",keep = False, inplace = True)\n",
    "\n",
    "def optimizasyon(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    noktalamaIsaretleri = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "    stop_words.update(noktalamaIsaretleri)\n",
    "\n",
    "    for ind in dataset.index:\n",
    "        body = dataset['Body'][ind]\n",
    "        body = body.lower()\n",
    "        body = re.sub(r'http\\S+', '', body)\n",
    "        body = re.sub('\\[[^]]*\\]', '', body)\n",
    "        body = (\" \").join([word for word in body.split() if not word in stop_words])\n",
    "        body = \"\".join([char for char in body if not char in noktalamaIsaretleri])\n",
    "        dataset['Body'][ind] = body\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = optimizasyon(dataset)\n",
    "\n",
    "\n",
    "pat1 = '@[^ ]+'\n",
    "pat2 = 'http[^ ]+'\n",
    "pat3 = 'www.[^ ]+'\n",
    "pat4 = '#[^ ]+'\n",
    "pat5 = '[0-9]'\n",
    "\n",
    "combined_pat = '|'.join((pat1, pat2, pat3, pat4, pat5))\n",
    "\n",
    "\n",
    "for ind in dataset.index:\n",
    "    t = dataset['Body'][ind]\n",
    "    t = t.lower()\n",
    "    stripped = re.sub(combined_pat, '', t)\n",
    "    tokens = word_tokenize(stripped)\n",
    "    words = [x for x  in tokens if len(x) > 1]\n",
    "    sentences = \" \".join(words)\n",
    "    dataset['Body'][ind] = sentences\n",
    "\n",
    "x = dataset['Body']\n",
    "y = dataset['Label']\n",
    "\n",
    "\n",
    "tv = TfidfVectorizer(stop_words='turkish', binary=False, ngram_range=(1,3))\n",
    "x_tv = tv.fit_transform(x)\n",
    "x_train_tv, x_test_tv, y_train_tv, y_test_tv = train_test_split(x_tv, y, test_size=0.2, random_state=0)\n",
    "\n",
    "log_tv = LogisticRegression() \n",
    "log_tv.fit(x_train_tv,y_train_tv)\n",
    "\n",
    "y_pred_tv = log_tv.predict(x_test_tv)\n",
    "print(confusion_matrix(y_test_tv,y_pred_tv))\n",
    "print(classification_report(y_test_tv,y_pred_tv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lojistik Regrasyon Algoritması ile Sistemin Çalıştırılma Çıktıları</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/2-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/3-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/4-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/5-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://artemiseticaret.com/wp-content/uploads/2021/05/6-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4- Algoritmaların Karşılaştırılması (Sonuç)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote cite=\"https://www.huxley.net/bnw/four.html\">Sonuç olarak Lojistik Regrasyon algoritması için geliştirilen sistemin doğruluk oranı daha yüksektir. Lstm kullanarak sistemi eğitirken yaklaşık 15 dakikada model geliştirilmiştir. Lojistik regrasyon daha basit yapıda olduğu için 5 saniyede model oluşturulmuştur. Böyle bir proje gerçekleştirilirken maliyet açısından değerlendirilecek olursa <b>Lojistik regrasyonu</b> tercih edilmelidir. Ayrıca bu proje 0 ya da 1 gibi değerlere yakın olduğu için Lojistik Regrasyon daha uygun bir algoritmadır.</blockquote>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
